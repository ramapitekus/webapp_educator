{"ast":null,"code":"var _jsxFileName = \"C:\\\\Users\\\\olesr\\\\OneDrive\\\\Documents\\\\webapp_educ\\\\educator\\\\src\\\\S2t.js\";\nimport { jsxDEV as _jsxDEV } from \"react/jsx-dev-runtime\";\n\n//import { useRef } from \"react\";\nconst S2t = () => {\n  //var audioCtx = new (window.AudioContext || window.webkitAudioContext)();\n  //var source = audioCtx.createBufferSource();\n  //audioCtx.decodeAudioData(props.audio, function (buffer) {\n  //  source.buffer = buffer;\n  //\n  //  source.connect(audioCtx.destination);\n  //  source.loop = true;\n  //});\n  //const audioArray = useRef(null);\n  //async function convertToArray() {\n  //  const reader = new FileReader();\n  //  reader.onload = function (e) {\n  //    audioArray.current = e.target.result;\n  //  };\n  //  reader.readAsArrayBuffer(props.audio);\n  //}\n  //await convertToArray();\n  const sdk = require(\"microsoft-cognitiveservices-speech-sdk\");\n\n  const speechConfig = sdk.SpeechConfig.fromSubscription(\"453a4f6f9f194b9cb503930edaabb2d9\", \"eastus\");\n  speechConfig.speechRecognitionLanguage = \"de-CH\";\n  let audioConfig = sdk.AudioConfig.fromWavFileInput(\"./test.wav\");\n  let speechRecognizer = new sdk.SpeechRecognizer(speechConfig, audioConfig);\n  speechRecognizer.recognizeOnceAsync(result => {\n    switch (result.reason) {\n      case sdk.ResultReason.RecognizedSpeech:\n        console.log(`RECOGNIZED: Text=${result.text}`);\n        break;\n\n      case sdk.ResultReason.NoMatch:\n        console.log(\"NOMATCH: Speech could not be recognized.\");\n        break;\n\n      case sdk.ResultReason.Canceled:\n        //const cancellation = CancellationDetails.fromResult(result);\n        console.log(`CANCELED: Reason=`); //if (cancellation.reason == sdk.CancellationReason.Error) {\n        //  console.log(`CANCELED: ErrorCode=${cancellation.ErrorCode}`);\n        //  console.log(`CANCELED: ErrorDetails=${cancellation.errorDetails}`);\n        //  console.log(\n        //    \"CANCELED: Did you update the key and location/region info?\"\n        //  );\n        //}\n\n        break;\n    }\n\n    speechRecognizer.close();\n  });\n  return sdk && /*#__PURE__*/_jsxDEV(\"h1\", {\n    children: \"works\"\n  }, void 0, false, {\n    fileName: _jsxFileName,\n    lineNumber: 57,\n    columnNumber: 17\n  }, this);\n};\n\n_c = S2t;\nexport default S2t;\n\nvar _c;\n\n$RefreshReg$(_c, \"S2t\");","map":{"version":3,"sources":["C:/Users/olesr/OneDrive/Documents/webapp_educ/educator/src/S2t.js"],"names":["S2t","sdk","require","speechConfig","SpeechConfig","fromSubscription","speechRecognitionLanguage","audioConfig","AudioConfig","fromWavFileInput","speechRecognizer","SpeechRecognizer","recognizeOnceAsync","result","reason","ResultReason","RecognizedSpeech","console","log","text","NoMatch","Canceled","close"],"mappings":";;;AAAA;AAEA,MAAMA,GAAG,GAAG,MAAM;AAChB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA,QAAMC,GAAG,GAAGC,OAAO,CAAC,wCAAD,CAAnB;;AACA,QAAMC,YAAY,GAAGF,GAAG,CAACG,YAAJ,CAAiBC,gBAAjB,CACnB,kCADmB,EAEnB,QAFmB,CAArB;AAIAF,EAAAA,YAAY,CAACG,yBAAb,GAAyC,OAAzC;AAEA,MAAIC,WAAW,GAAGN,GAAG,CAACO,WAAJ,CAAgBC,gBAAhB,CAAiC,YAAjC,CAAlB;AACA,MAAIC,gBAAgB,GAAG,IAAIT,GAAG,CAACU,gBAAR,CAAyBR,YAAzB,EAAuCI,WAAvC,CAAvB;AAEAG,EAAAA,gBAAgB,CAACE,kBAAjB,CAAqCC,MAAD,IAAY;AAC9C,YAAQA,MAAM,CAACC,MAAf;AACE,WAAKb,GAAG,CAACc,YAAJ,CAAiBC,gBAAtB;AACEC,QAAAA,OAAO,CAACC,GAAR,CAAa,oBAAmBL,MAAM,CAACM,IAAK,EAA5C;AACA;;AACF,WAAKlB,GAAG,CAACc,YAAJ,CAAiBK,OAAtB;AACEH,QAAAA,OAAO,CAACC,GAAR,CAAY,0CAAZ;AACA;;AACF,WAAKjB,GAAG,CAACc,YAAJ,CAAiBM,QAAtB;AACE;AACAJ,QAAAA,OAAO,CAACC,GAAR,CAAa,mBAAb,EAFF,CAIE;AACA;AACA;AACA;AACA;AACA;AACA;;AACA;AAlBJ;;AAoBAR,IAAAA,gBAAgB,CAACY,KAAjB;AACD,GAtBD;AAwBA,SAAOrB,GAAG,iBAAI;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,UAAd;AACD,CAvDD;;KAAMD,G;AAyDN,eAAeA,GAAf","sourcesContent":["//import { useRef } from \"react\";\r\n\r\nconst S2t = () => {\r\n  //var audioCtx = new (window.AudioContext || window.webkitAudioContext)();\r\n  //var source = audioCtx.createBufferSource();\r\n  //audioCtx.decodeAudioData(props.audio, function (buffer) {\r\n  //  source.buffer = buffer;\r\n  //\r\n  //  source.connect(audioCtx.destination);\r\n  //  source.loop = true;\r\n  //});\r\n  //const audioArray = useRef(null);\r\n\r\n  //async function convertToArray() {\r\n  //  const reader = new FileReader();\r\n  //  reader.onload = function (e) {\r\n  //    audioArray.current = e.target.result;\r\n  //  };\r\n  //  reader.readAsArrayBuffer(props.audio);\r\n  //}\r\n\r\n  //await convertToArray();\r\n  const sdk = require(\"microsoft-cognitiveservices-speech-sdk\");\r\n  const speechConfig = sdk.SpeechConfig.fromSubscription(\r\n    \"453a4f6f9f194b9cb503930edaabb2d9\",\r\n    \"eastus\"\r\n  );\r\n  speechConfig.speechRecognitionLanguage = \"de-CH\";\r\n\r\n  let audioConfig = sdk.AudioConfig.fromWavFileInput(\"./test.wav\");\r\n  let speechRecognizer = new sdk.SpeechRecognizer(speechConfig, audioConfig);\r\n\r\n  speechRecognizer.recognizeOnceAsync((result) => {\r\n    switch (result.reason) {\r\n      case sdk.ResultReason.RecognizedSpeech:\r\n        console.log(`RECOGNIZED: Text=${result.text}`);\r\n        break;\r\n      case sdk.ResultReason.NoMatch:\r\n        console.log(\"NOMATCH: Speech could not be recognized.\");\r\n        break;\r\n      case sdk.ResultReason.Canceled:\r\n        //const cancellation = CancellationDetails.fromResult(result);\r\n        console.log(`CANCELED: Reason=`);\r\n\r\n        //if (cancellation.reason == sdk.CancellationReason.Error) {\r\n        //  console.log(`CANCELED: ErrorCode=${cancellation.ErrorCode}`);\r\n        //  console.log(`CANCELED: ErrorDetails=${cancellation.errorDetails}`);\r\n        //  console.log(\r\n        //    \"CANCELED: Did you update the key and location/region info?\"\r\n        //  );\r\n        //}\r\n        break;\r\n    }\r\n    speechRecognizer.close();\r\n  });\r\n\r\n  return sdk && <h1>works</h1>;\r\n};\r\n\r\nexport default S2t;\r\n"]},"metadata":{},"sourceType":"module"}