{"ast":null,"code":"var _jsxFileName = \"C:\\\\Users\\\\olesr\\\\OneDrive\\\\Documents\\\\webapp_educ\\\\educator\\\\src\\\\S2t.js\",\n    _s = $RefreshSig$();\n\nimport { useState } from \"react\";\nimport { jsxDEV as _jsxDEV } from \"react/jsx-dev-runtime\";\n\nconst S2t = async props => {\n  _s();\n\n  const [audioArray, setAudioArray] = useState(null);\n\n  async function convertToArray() {\n    const reader = new FileReader();\n\n    reader.onload = function (e) {\n      setAudioArray(e.target.result);\n    };\n\n    reader.readAsArrayBuffer(props.audio);\n  }\n\n  await convertToArray();\n\n  const sdk = require(\"microsoft-cognitiveservices-speech-sdk\");\n\n  const speechConfig = sdk.SpeechConfig.fromSubscription(\"453a4f6f9f194b9cb503930edaabb2d9\", \"eastus\");\n  speechConfig.speechRecognitionLanguage = \"de-CH\";\n  let audioConfig = sdk.AudioConfig.fromWavFileInput(audioArray);\n  let speechRecognizer = new sdk.SpeechRecognizer(speechConfig, audioConfig); //speechRecognizer.recognizeOnceAsync((result) => {\n  //  switch (result.reason) {\n  //    case sdk.ResultReason.RecognizedSpeech:\n  //      console.log(`RECOGNIZED: Text=${result.text}`);\n  //      break;\n  //    case sdk.ResultReason.NoMatch:\n  //      console.log(\"NOMATCH: Speech could not be recognized.\");\n  //      break;\n  //    case sdk.ResultReason.Canceled:\n  //      const cancellation = CancellationDetails.fromResult(result);\n  //      console.log(`CANCELED: Reason=${cancellation.reason}`);\n  //\n  //      if (cancellation.reason == sdk.CancellationReason.Error) {\n  //        console.log(`CANCELED: ErrorCode=${cancellation.ErrorCode}`);\n  //        console.log(`CANCELED: ErrorDetails=${cancellation.errorDetails}`);\n  //        console.log(\n  //          \"CANCELED: Did you update the key and location/region info?\"\n  //        );\n  //      }\n  //      break;\n  //  }\n  //  speechRecognizer.close();\n  //});\n\n  return sdk && /*#__PURE__*/_jsxDEV(\"h1\", {\n    children: \"works\"\n  }, void 0, false, {\n    fileName: _jsxFileName,\n    lineNumber: 49,\n    columnNumber: 17\n  }, this);\n};\n\n_s(S2t, \"SXgGaRigzRjpfyCYieVrSytRkms=\");\n\n_c = S2t;\nexport default S2t;\n\nvar _c;\n\n$RefreshReg$(_c, \"S2t\");","map":{"version":3,"sources":["C:/Users/olesr/OneDrive/Documents/webapp_educ/educator/src/S2t.js"],"names":["useState","S2t","props","audioArray","setAudioArray","convertToArray","reader","FileReader","onload","e","target","result","readAsArrayBuffer","audio","sdk","require","speechConfig","SpeechConfig","fromSubscription","speechRecognitionLanguage","audioConfig","AudioConfig","fromWavFileInput","speechRecognizer","SpeechRecognizer"],"mappings":";;;AAAA,SAASA,QAAT,QAAyB,OAAzB;;;AAEA,MAAMC,GAAG,GAAG,MAAOC,KAAP,IAAiB;AAAA;;AAC3B,QAAM,CAACC,UAAD,EAAaC,aAAb,IAA8BJ,QAAQ,CAAC,IAAD,CAA5C;;AAEA,iBAAeK,cAAf,GAAgC;AAC9B,UAAMC,MAAM,GAAG,IAAIC,UAAJ,EAAf;;AACAD,IAAAA,MAAM,CAACE,MAAP,GAAgB,UAAUC,CAAV,EAAa;AAC3BL,MAAAA,aAAa,CAACK,CAAC,CAACC,MAAF,CAASC,MAAV,CAAb;AACD,KAFD;;AAGAL,IAAAA,MAAM,CAACM,iBAAP,CAAyBV,KAAK,CAACW,KAA/B;AACD;;AAED,QAAMR,cAAc,EAApB;;AACA,QAAMS,GAAG,GAAGC,OAAO,CAAC,wCAAD,CAAnB;;AACA,QAAMC,YAAY,GAAGF,GAAG,CAACG,YAAJ,CAAiBC,gBAAjB,CACnB,kCADmB,EAEnB,QAFmB,CAArB;AAIAF,EAAAA,YAAY,CAACG,yBAAb,GAAyC,OAAzC;AAEA,MAAIC,WAAW,GAAGN,GAAG,CAACO,WAAJ,CAAgBC,gBAAhB,CAAiCnB,UAAjC,CAAlB;AACA,MAAIoB,gBAAgB,GAAG,IAAIT,GAAG,CAACU,gBAAR,CAAyBR,YAAzB,EAAuCI,WAAvC,CAAvB,CApB2B,CAsB3B;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,SAAON,GAAG,iBAAI;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,UAAd;AACD,CA/CD;;GAAMb,G;;KAAAA,G;AAiDN,eAAeA,GAAf","sourcesContent":["import { useState } from \"react\";\r\n\r\nconst S2t = async (props) => {\r\n  const [audioArray, setAudioArray] = useState(null);\r\n\r\n  async function convertToArray() {\r\n    const reader = new FileReader();\r\n    reader.onload = function (e) {\r\n      setAudioArray(e.target.result);\r\n    };\r\n    reader.readAsArrayBuffer(props.audio);\r\n  }\r\n\r\n  await convertToArray();\r\n  const sdk = require(\"microsoft-cognitiveservices-speech-sdk\");\r\n  const speechConfig = sdk.SpeechConfig.fromSubscription(\r\n    \"453a4f6f9f194b9cb503930edaabb2d9\",\r\n    \"eastus\"\r\n  );\r\n  speechConfig.speechRecognitionLanguage = \"de-CH\";\r\n\r\n  let audioConfig = sdk.AudioConfig.fromWavFileInput(audioArray);\r\n  let speechRecognizer = new sdk.SpeechRecognizer(speechConfig, audioConfig);\r\n\r\n  //speechRecognizer.recognizeOnceAsync((result) => {\r\n  //  switch (result.reason) {\r\n  //    case sdk.ResultReason.RecognizedSpeech:\r\n  //      console.log(`RECOGNIZED: Text=${result.text}`);\r\n  //      break;\r\n  //    case sdk.ResultReason.NoMatch:\r\n  //      console.log(\"NOMATCH: Speech could not be recognized.\");\r\n  //      break;\r\n  //    case sdk.ResultReason.Canceled:\r\n  //      const cancellation = CancellationDetails.fromResult(result);\r\n  //      console.log(`CANCELED: Reason=${cancellation.reason}`);\r\n  //\r\n  //      if (cancellation.reason == sdk.CancellationReason.Error) {\r\n  //        console.log(`CANCELED: ErrorCode=${cancellation.ErrorCode}`);\r\n  //        console.log(`CANCELED: ErrorDetails=${cancellation.errorDetails}`);\r\n  //        console.log(\r\n  //          \"CANCELED: Did you update the key and location/region info?\"\r\n  //        );\r\n  //      }\r\n  //      break;\r\n  //  }\r\n  //  speechRecognizer.close();\r\n  //});\r\n\r\n  return sdk && <h1>works</h1>;\r\n};\r\n\r\nexport default S2t;\r\n"]},"metadata":{},"sourceType":"module"}