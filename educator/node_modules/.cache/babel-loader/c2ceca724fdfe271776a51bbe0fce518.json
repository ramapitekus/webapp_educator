{"ast":null,"code":"var _jsxFileName = \"C:\\\\Users\\\\olesr\\\\OneDrive\\\\Documents\\\\webapp_educ\\\\educator\\\\src\\\\S2t.js\";\nimport { jsxDEV as _jsxDEV } from \"react/jsx-dev-runtime\";\n\n//import { useRef } from \"react\";\nconst S2t = props => {\n  var audioCtx = new (window.AudioContext || window.webkitAudioContext)(); //const audioArray = useRef(null);\n  //async function convertToArray() {\n  //  const reader = new FileReader();\n  //  reader.onload = function (e) {\n  //    audioArray.current = e.target.result;\n  //  };\n  //  reader.readAsArrayBuffer(props.audio);\n  //}\n  //await convertToArray();\n\n  const sdk = require(\"microsoft-cognitiveservices-speech-sdk\");\n\n  const speechConfig = sdk.SpeechConfig.fromSubscription(\"453a4f6f9f194b9cb503930edaabb2d9\", \"eastus\");\n  console.log(props.audio);\n  speechConfig.speechRecognitionLanguage = \"de-CH\";\n  let audioConfig = sdk.AudioConfig.fromWavFileInput(props.audio); //let speechRecognizer = new sdk.SpeechRecognizer(speechConfig, audioConfig);\n  //console.log(audioConfig);\n  //speechRecognizer.recognizeOnceAsync((result) => {\n  //  switch (result.reason) {\n  //    case sdk.ResultReason.RecognizedSpeech:\n  //      console.log(`RECOGNIZED: Text=${result.text}`);\n  //      break;\n  //    case sdk.ResultReason.NoMatch:\n  //      console.log(\"NOMATCH: Speech could not be recognized.\");\n  //      break;\n  //    case sdk.ResultReason.Canceled:\n  //      const cancellation = CancellationDetails.fromResult(result);\n  //      console.log(`CANCELED: Reason=${cancellation.reason}`);\n  //\n  //      if (cancellation.reason == sdk.CancellationReason.Error) {\n  //        console.log(`CANCELED: ErrorCode=${cancellation.ErrorCode}`);\n  //        console.log(`CANCELED: ErrorDetails=${cancellation.errorDetails}`);\n  //        console.log(\n  //          \"CANCELED: Did you update the key and location/region info?\"\n  //        );\n  //      }\n  //      break;\n  //  }\n  //  speechRecognizer.close();\n  //});\n\n  return sdk && /*#__PURE__*/_jsxDEV(\"h1\", {\n    children: \"works\"\n  }, void 0, false, {\n    fileName: _jsxFileName,\n    lineNumber: 52,\n    columnNumber: 17\n  }, this);\n};\n\n_c = S2t;\nexport default S2t;\n\nvar _c;\n\n$RefreshReg$(_c, \"S2t\");","map":{"version":3,"sources":["C:/Users/olesr/OneDrive/Documents/webapp_educ/educator/src/S2t.js"],"names":["S2t","props","audioCtx","window","AudioContext","webkitAudioContext","sdk","require","speechConfig","SpeechConfig","fromSubscription","console","log","audio","speechRecognitionLanguage","audioConfig","AudioConfig","fromWavFileInput"],"mappings":";;;AAAA;AAEA,MAAMA,GAAG,GAAIC,KAAD,IAAW;AACrB,MAAIC,QAAQ,GAAG,KAAKC,MAAM,CAACC,YAAP,IAAuBD,MAAM,CAACE,kBAAnC,GAAf,CADqB,CAErB;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA;;AACA,QAAMC,GAAG,GAAGC,OAAO,CAAC,wCAAD,CAAnB;;AACA,QAAMC,YAAY,GAAGF,GAAG,CAACG,YAAJ,CAAiBC,gBAAjB,CACnB,kCADmB,EAEnB,QAFmB,CAArB;AAIAC,EAAAA,OAAO,CAACC,GAAR,CAAYX,KAAK,CAACY,KAAlB;AACAL,EAAAA,YAAY,CAACM,yBAAb,GAAyC,OAAzC;AAEA,MAAIC,WAAW,GAAGT,GAAG,CAACU,WAAJ,CAAgBC,gBAAhB,CAAiChB,KAAK,CAACY,KAAvC,CAAlB,CArBqB,CAsBrB;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,SAAOP,GAAG,iBAAI;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,UAAd;AACD,CAlDD;;KAAMN,G;AAoDN,eAAeA,GAAf","sourcesContent":["//import { useRef } from \"react\";\r\n\r\nconst S2t = (props) => {\r\n  var audioCtx = new (window.AudioContext || window.webkitAudioContext)();\r\n  //const audioArray = useRef(null);\r\n\r\n  //async function convertToArray() {\r\n  //  const reader = new FileReader();\r\n  //  reader.onload = function (e) {\r\n  //    audioArray.current = e.target.result;\r\n  //  };\r\n  //  reader.readAsArrayBuffer(props.audio);\r\n  //}\r\n\r\n  //await convertToArray();\r\n  const sdk = require(\"microsoft-cognitiveservices-speech-sdk\");\r\n  const speechConfig = sdk.SpeechConfig.fromSubscription(\r\n    \"453a4f6f9f194b9cb503930edaabb2d9\",\r\n    \"eastus\"\r\n  );\r\n  console.log(props.audio);\r\n  speechConfig.speechRecognitionLanguage = \"de-CH\";\r\n\r\n  let audioConfig = sdk.AudioConfig.fromWavFileInput(props.audio);\r\n  //let speechRecognizer = new sdk.SpeechRecognizer(speechConfig, audioConfig);\r\n  //console.log(audioConfig);\r\n\r\n  //speechRecognizer.recognizeOnceAsync((result) => {\r\n  //  switch (result.reason) {\r\n  //    case sdk.ResultReason.RecognizedSpeech:\r\n  //      console.log(`RECOGNIZED: Text=${result.text}`);\r\n  //      break;\r\n  //    case sdk.ResultReason.NoMatch:\r\n  //      console.log(\"NOMATCH: Speech could not be recognized.\");\r\n  //      break;\r\n  //    case sdk.ResultReason.Canceled:\r\n  //      const cancellation = CancellationDetails.fromResult(result);\r\n  //      console.log(`CANCELED: Reason=${cancellation.reason}`);\r\n  //\r\n  //      if (cancellation.reason == sdk.CancellationReason.Error) {\r\n  //        console.log(`CANCELED: ErrorCode=${cancellation.ErrorCode}`);\r\n  //        console.log(`CANCELED: ErrorDetails=${cancellation.errorDetails}`);\r\n  //        console.log(\r\n  //          \"CANCELED: Did you update the key and location/region info?\"\r\n  //        );\r\n  //      }\r\n  //      break;\r\n  //  }\r\n  //  speechRecognizer.close();\r\n  //});\r\n\r\n  return sdk && <h1>works</h1>;\r\n};\r\n\r\nexport default S2t;\r\n"]},"metadata":{},"sourceType":"module"}